# ğŸš€ ETL Warehouse - Pipeline Sneakers Temps RÃ©el

> **Infrastructure complÃ¨te de donnÃ©es pour e-commerce sneakers avec pipeline streaming, monitoring et rapports automatisÃ©s**

[![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![Kafka](https://img.shields.io/badge/Apache_Kafka-231F20?style=for-the-badge&logo=apache-kafka&logoColor=white)](https://kafka.apache.org/)
[![Snowflake](https://img.shields.io/badge/Snowflake-29B5E8?style=for-the-badge&logo=snowflake&logoColor=white)](https://www.snowflake.com/)
[![Airflow](https://img.shields.io/badge/Apache_Airflow-017CEE?style=for-the-badge&logo=apache-airflow&logoColor=white)](https://airflow.apache.org/)
[![Grafana](https://img.shields.io/badge/Grafana-F46800?style=for-the-badge&logo=grafana&logoColor=white)](https://grafana.com/)
[![Prometheus](https://img.shields.io/badge/Prometheus-E6522C?style=for-the-badge&logo=prometheus&logoColor=white)](https://prometheus.io/)

## ğŸ¯ **Vue d'ensemble**

Ce projet implÃ©mente un **pipeline ETL moderne** pour un e-commerce de sneakers, combinant :
- **Streaming temps rÃ©el** (Kafka â†’ Snowflake)
- **Monitoring avancÃ©** (Prometheus + Grafana)  
- **Rapports automatisÃ©s** (Airflow)
- **DonnÃ©es rÃ©alistes** (gÃ©nÃ©rateur de sneakers avec 8 marques)

## ğŸ—ï¸ **Architecture**

```
ğŸ“± Producer Python â†’ ğŸ“¨ Kafka/Redpanda â†’ ğŸ Consumer Python â†’ â„ï¸ Snowflake
     â†“                    â†“                    â†“                    â†“
ğŸ“Š Prometheus â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â†
     â†“                                                              â†“
ğŸ“ˆ Grafana                                                   ğŸ”„ Airflow
```

## ğŸš€ **DÃ©marrage Rapide**

### PrÃ©requis
- Docker & Docker Compose
- Compte Snowflake actif
- 8GB RAM minimum

### 1. Configuration Snowflake
```bash
# Configurer les variables d'environnement
export SNOWFLAKE_ACCOUNT="votre-account"
export SNOWFLAKE_USER="votre-user"  
export SNOWFLAKE_PASSWORD="votre-password"
```

### 2. Lancement du pipeline complet
```bash
# Cloner le repository
git clone https://github.com/M13E-LAB/AutomationSneakFreak.git
cd AutomationSneakFreak

# Construire l'image Python
docker build -t kafka-lab:latest .

# Lancer l'infrastructure complÃ¨te
docker-compose up -d

# CrÃ©er le topic Kafka
docker exec redpanda rpk topic create sneaker-orders --partitions 3

# VÃ©rifier les logs
docker-compose logs -f producer consumer
```

## ğŸŒ **Services Disponibles**

| Service | URL | Credentials | Description |
|---------|-----|-------------|-------------|
| **Grafana** | http://localhost:3000 | admin/admin123 | Dashboards & monitoring |
| **Airflow** | http://localhost:8081 | admin/admin123 | Orchestration & rapports |
| **Prometheus** | http://localhost:9090 | - | MÃ©triques systÃ¨me |
| **Redpanda Console** | http://localhost:8080 | - | Interface Kafka |
| **Producer Metrics** | http://localhost:8001/metrics | - | MÃ©triques producer |
| **Consumer Metrics** | http://localhost:8002/metrics | - | MÃ©triques consumer |

## ğŸ“Š **DonnÃ©es GÃ©nÃ©rÃ©es**

### Marques de Sneakers (rÃ©alistes)
- **Nike** : Air Max, Air Force 1, Dunk, Jordan, React, Zoom, Blazer, Cortez
- **Adidas** : Stan Smith, Superstar, Gazelle, Ultra Boost, NMD, Samba, Yeezy
- **New Balance** : 990, 574, 327, 2002R, 550, Fresh Foam, 9060
- **Converse** : Chuck Taylor, One Star, Pro Leather, Jack Purcell
- **Vans** : Old Skool, Authentic, Era, Sk8-Hi, Slip-On, Knu Skool
- **Puma** : Suede, Clyde, RS-X, Speedcat, Palermo, Easy Rider
- **Asics** : Gel-Lyte III, Gel-Kayano, Japan S, Mexico 66, Gel-Nimbus
- **Reebok** : Club C, Classic Leather, Pump, Instapump Fury, Question

### Exemple de commande gÃ©nÃ©rÃ©e
```json
{
  "event_type": "order_created",
  "timestamp": "2025-01-15T14:30:45.123Z",
  "order_id": 12345,
  "customer_name": "Marie Dubois",
  "customer_city": "Paris",
  "brand": "Nike",
  "model": "Air Max 90",
  "color": "White",
  "size": "42",
  "unit_price": 120.50,
  "total_amount": 241.00,
  "quantity": 2
}
```

## ğŸ“ˆ **MÃ©triques CollectÃ©es**

### Producer (Port 8001)
- `sneaker_orders_sent_total` - Commandes envoyÃ©es par marque
- `sneaker_orders_sent_duration_seconds` - Latence d'envoi Kafka
- `sneaker_kafka_errors_total` - Erreurs Kafka
- `sneaker_orders_per_minute` - Taux de gÃ©nÃ©ration

### Consumer (Port 8002)  
- `sneaker_orders_consumed_total` - Commandes reÃ§ues par marque
- `sneaker_orders_loaded_snowflake_total` - Commandes chargÃ©es
- `sneaker_snowflake_load_duration_seconds` - Latence Snowflake
- `sneaker_orders_processing_per_minute` - Taux de traitement

## ğŸ”„ **DAGs Airflow**

### 1. `daily_sneaker_report` (quotidien Ã  8h)
- Extraction des donnÃ©es Snowflake (24h)
- GÃ©nÃ©ration de rapport HTML avec KPIs
- MÃ©triques JSON pour Grafana
- Top marques, modÃ¨les, tailles populaires

### 2. `weekly_sneaker_trends` (hebdomadaire, lundi 9h)
- Analyse des tendances sur 7 jours
- Graphiques matplotlib (Ã©volution, patterns)
- DÃ©tection des marques en croissance
- Analyse gÃ©ographique et temporelle

## ğŸ—„ï¸ **SchÃ©ma Snowflake**

### Tables crÃ©Ã©es automatiquement
```sql
-- DonnÃ©es de rÃ©fÃ©rence (batch)
RETAIL_LAB.STG.CUSTOMERS     -- Clients
RETAIL_LAB.STG.INVENTORY     -- Inventaire sneakers  
RETAIL_LAB.STG.ORDERS        -- Commandes batch

-- DonnÃ©es streaming (temps rÃ©el)
RETAIL_LAB.STG.STREAMING_ORDERS  -- Commandes temps rÃ©el
```

### Exemple de requÃªte d'analyse
```sql
-- Top 5 marques par CA (derniÃ¨res 24h)
SELECT 
    BRAND,
    COUNT(*) as orders_count,
    SUM(TOTAL_AMOUNT) as revenue,
    AVG(TOTAL_AMOUNT) as avg_order_value
FROM RETAIL_LAB.STG.STREAMING_ORDERS 
WHERE PROCESSED_AT >= CURRENT_DATE - 1
GROUP BY BRAND
ORDER BY revenue DESC
LIMIT 5;
```

## ğŸ“ **Structure du Projet**

```
ETL-Warehouse/
â”œâ”€â”€ ğŸ“Š monitoring/
â”‚   â”œâ”€â”€ prometheus.yml          # Config Prometheus
â”‚   â””â”€â”€ grafana/
â”‚       â”œâ”€â”€ provisioning/       # Sources de donnÃ©es
â”‚       â””â”€â”€ dashboards/         # Dashboards JSON
â”œâ”€â”€ ğŸ”„ airflow/
â”‚   â”œâ”€â”€ dags/                   # DAGs Python
â”‚   â”œâ”€â”€ logs/                   # Logs Airflow
â”‚   â””â”€â”€ plugins/                # Plugins custom
â”œâ”€â”€ ğŸ src/
â”‚   â”œâ”€â”€ producer_order_status.py    # GÃ©nÃ©rateur commandes
â”‚   â””â”€â”€ consumer_to_snowflake.py    # Chargeur Snowflake
â”œâ”€â”€ ğŸ“‹ reports/                 # Rapports gÃ©nÃ©rÃ©s
â”œâ”€â”€ ğŸ³ docker-compose.yml      # Orchestration complÃ¨te
â”œâ”€â”€ ğŸ”§ Dockerfile              # Image Python
â”œâ”€â”€ ğŸ“Š data_generator.py       # GÃ©nÃ©rateur donnÃ©es
â””â”€â”€ â„ï¸ load_to_snowflake.py   # Chargement initial
```

## ğŸ”§ **Configuration AvancÃ©e**

### Variables d'environnement
```bash
# Snowflake (requis)
SNOWFLAKE_ACCOUNT=gupyoza-zq65095
SNOWFLAKE_USER=ANOUAR
SNOWFLAKE_PASSWORD=***
SNOWFLAKE_WAREHOUSE=TEACH_WH
SNOWFLAKE_DATABASE=RETAIL_LAB
SNOWFLAKE_SCHEMA=STG

# Kafka
KAFKA_TOPIC_NAME=sneaker-orders
KAFKA_BOOTSTRAP=redpanda:9092

# Airflow
AIRFLOW__CORE__FERNET_KEY=***
```

### Personnalisation du gÃ©nÃ©rateur
```python
# Modifier data_generator.py pour :
- Ajouter de nouvelles marques
- Changer la distribution des prix  
- Modifier la frÃ©quence de gÃ©nÃ©ration
- Personnaliser les tailles disponibles
```

## ğŸ“Š **Dashboards Grafana**

### Dashboard Principal : "Sneaker Pipeline Monitoring"
- **ğŸ“¨ Orders Sent Rate** - Taux d'envoi temps rÃ©el
- **ğŸ“¥ Orders Consumed Rate** - Taux de consommation  
- **ğŸ“Š Total Orders** - Compteurs cumulÃ©s
- **â±ï¸ Latency Metrics** - Performance Snowflake
- **ğŸ‘Ÿ Orders by Brand** - RÃ©partition par marque
- **âŒ Error Tracking** - Suivi des erreurs

## ğŸš¨ **Monitoring & Alertes**

### MÃ©triques clÃ©s Ã  surveiller
- **Latence Snowflake** > 5 secondes
- **Erreurs Kafka** > 0
- **Taux de traitement** < 10 commandes/minute
- **Espace disque** Prometheus/Grafana

### Logs importants
```bash
# Logs producer/consumer
docker-compose logs -f producer consumer

# Logs Airflow
docker-compose logs -f airflow-scheduler airflow-webserver

# MÃ©triques en direct
curl http://localhost:8001/metrics | grep sneaker
```

## ğŸ”„ **Commandes Utiles**

```bash
# RedÃ©marrage complet
docker-compose down && docker-compose up -d

# Reconstruction de l'image
docker build -t kafka-lab:latest . && docker-compose restart producer consumer

# Nettoyage des volumes
docker-compose down -v

# Backup des rapports
docker cp airflow-webserver:/opt/airflow/reports ./backup-reports

# Monitoring des ressources
docker stats

# VÃ©rification des topics Kafka
docker exec redpanda rpk topic list
docker exec redpanda rpk topic describe sneaker-orders
```

## ğŸ¯ **Cas d'Usage**

### 1. **E-commerce Analytics**
- Suivi temps rÃ©el des ventes
- DÃ©tection des tendances produits
- Optimisation des stocks
- Analyse comportementale clients

### 2. **Monitoring OpÃ©rationnel**  
- Performance du pipeline
- DÃ©tection d'anomalies
- Alertes automatiques
- Tableaux de bord executives

### 3. **Rapports Business**
- KPIs quotidiens automatisÃ©s
- Analyses hebdomadaires
- Comparaisons pÃ©riode sur pÃ©riode
- Insights gÃ©ographiques

## ğŸ› ï¸ **DÃ©veloppement**

### Ajouter une nouvelle mÃ©trique
```python
# Dans producer_order_status.py
new_metric = Counter('sneaker_new_metric_total', 'Description')
new_metric.labels(label='value').inc()
```

### CrÃ©er un nouveau DAG
```python
# Dans airflow/dags/
from airflow import DAG
from datetime import datetime

dag = DAG(
    'custom_sneaker_analysis',
    schedule_interval='@daily',
    start_date=datetime(2025, 1, 1)
)
```

### Personnaliser les dashboards
1. Modifier `monitoring/grafana/dashboards/*.json`
2. RedÃ©marrer Grafana : `docker-compose restart grafana`
3. Ou crÃ©er via l'interface web et exporter

## ğŸ¤ **Contribution**

1. Fork le repository
2. CrÃ©er une branche feature (`git checkout -b feature/amazing-feature`)
3. Commit les changements (`git commit -m 'Add amazing feature'`)
4. Push vers la branche (`git push origin feature/amazing-feature`)
5. Ouvrir une Pull Request

## ğŸ“ **Licence**

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ™ **Remerciements**

- **Apache Kafka** pour le streaming
- **Snowflake** pour l'entrepÃ´t de donnÃ©es  
- **Apache Airflow** pour l'orchestration
- **Prometheus & Grafana** pour le monitoring
- **Redpanda** pour l'interface Kafka simplifiÃ©e

---

**DÃ©veloppÃ© avec â¤ï¸ pour l'Ã©cosystÃ¨me data moderne**
